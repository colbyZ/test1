{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 4\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, October 5th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW4.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zambalayev, Timur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CSCI E-109a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Variable selection and regularization\n",
    "\n",
    "The data set for this problem is provided in ``dataset_1.txt`` and contains 10 predictors and a response variable.\n",
    "\n",
    "### Part (a): Analyze correlation among predictors\n",
    "- By visually inspecting the data set, do find that some of the predictors are correlated amongst themselves?\n",
    "\n",
    "\n",
    "- Compute the cofficient of correlation between each pair of predictors, and visualize the matrix of correlation coefficients using a heat map. Do the predictors fall naturally into groups based on the correlation values?\n",
    "\n",
    "\n",
    "- If you were asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, how many predictors will you pick, and which ones will you choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 rows:\n",
      "[[ 0.95935688  0.95935688  0.95935688  0.34372695  0.52408328  0.53776781\n",
      "   0.43559824  0.83199927  0.15324701  0.00501577]\n",
      " [ 0.6169695   0.6169695   0.6169695   0.28737557  0.51384368  0.49777511\n",
      "   0.45273244  0.91460929  0.36738984  0.444473  ]\n",
      " [ 0.99594053  0.99594053  0.99594053  0.10729412  0.0971065   0.14675084\n",
      "   0.13641377  0.63592648  0.5352095   0.89945713]\n",
      " [ 0.82173155  0.82173155  0.82173155  0.20255776  0.32950426  0.35947128\n",
      "   0.28145288  0.10626341  0.47932701  0.25627126]\n",
      " [ 0.30242269  0.30242269  0.30242269  0.18456376  0.27026295  0.29338515\n",
      "   0.26386634  0.37862981  0.74024145  0.46858931]\n",
      " [ 0.03763453  0.03763453  0.03763453  0.33779404  0.61511664  0.56764403\n",
      "   0.58995459  0.64830983  0.06158611  0.56185078]\n",
      " [ 0.54811654  0.54811654  0.54811654  0.11626237  0.11359164  0.08762936\n",
      "   0.11138565  0.9282435   0.12174737  0.22162753]\n",
      " [ 0.89282671  0.89282671  0.89282671  0.28155135  0.41668107  0.34884158\n",
      "   0.3518005   0.59422044  0.71221859  0.88587581]\n",
      " [ 0.74263799  0.74263799  0.74263799  0.3462708   0.50147076  0.46148795\n",
      "   0.47303945  0.15411911  0.75418325  0.61593037]\n",
      " [ 0.01902749  0.01902749  0.01902749  0.09067542  0.09267384  0.07781907\n",
      "   0.09477031  0.29587777  0.70593236  0.86574139]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAINCAYAAACuzwxgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/1RSzM0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3342358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Dataset_1_Data = namedtuple('Dataset_1_Data', ['x', 'y'])\n",
    "\n",
    "\n",
    "def load_dataset_1():\n",
    "    # Load data\n",
    "    data = np.loadtxt('datasets/dataset_1.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "    # Split predictors and response\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return Dataset_1_Data(x, y)\n",
    "\n",
    "\n",
    "def heatmap_prob_1a(dataset_1_data):\n",
    "    x = dataset_1_data.x\n",
    "    \n",
    "    print 'first 10 rows:\\n', x[:10]\n",
    "\n",
    "    # Compute matrix of correlation coefficients\n",
    "    corr_matrix = np.corrcoef(x.T)\n",
    "\n",
    "    # Display heat map\n",
    "    _, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    ax.pcolor(corr_matrix)\n",
    "    ax.set_title('Heatmap of correlation matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "dataset_1_data = load_dataset_1()\n",
    "heatmap_prob_1a(dataset_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***By visually inspecting the data set, do you find that some of the predictors are correlated amongst themselves?***\n",
    "\n",
    "You can notice that the first three predictors have the same value. For other predictors it's difficult to say just by looking at numbers.\n",
    "\n",
    "By examining the heatmap of the correlation matrix we can say that predictors x1, x2 and x3 are highly correlated to each other, the next four predictors (x4, x5, x6, x7) are also highly correlated to each other.\n",
    "\n",
    "***If you were asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, how many predictors will you pick, and which ones will you choose?***\n",
    "\n",
    "I would choose 5 predictors (one from {x1, x2, x3} group, one from {x4, x5, x6, x7} and the rest - x8, x9, x10). If we have a group of highly correlated to each other predictors, we need only one predictor from that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Selecting minimal subset of predictors\n",
    "\n",
    "- Apply the variable selection methods discussed in class to choose a minimal subset of predictors that yield high prediction accuracy:\n",
    "    \n",
    "    - Exhaustive search\n",
    "    \n",
    "    - Step-wise forward selection **or** Step-wise backward selection  \n",
    "\n",
    "&emsp;&nbsp;&nbsp; In each method, use the Bayesian Information Criterion (BIC) to choose the subset size.\n",
    "\n",
    "- Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?\n",
    "\n",
    "**Note**: You may use the `statsmodels`'s `OLS` module to fit a linear regression model and evaluate BIC. You may **not** use library functions that implement variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  1, bic: -36.720, R^2: 0.369, subset: (8,), \n",
      "k:  2, bic: -61.732, R^2: 0.531, subset: (8, 9), \n",
      "k:  3, bic: -65.729, R^2: 0.570, subset: (5, 8, 9), \n",
      "k:  4, bic: -71.008, R^2: 0.610, subset: (5, 7, 8, 9), \n",
      "k:  5, bic: -72.366, R^2: 0.633, subset: (0, 5, 7, 8, 9), \n",
      "k:  6, bic: -70.355, R^2: 0.642, subset: (0, 3, 5, 7, 8, 9), \n",
      "k:  7, bic: -66.159, R^2: 0.644, subset: (0, 3, 5, 6, 7, 8, 9), \n",
      "k:  8, bic: -61.803, R^2: 0.645, subset: (0, 3, 4, 5, 6, 7, 8, 9), \n",
      "k:  9, bic: -61.803, R^2: 0.645, subset: (0, 1, 3, 4, 5, 6, 7, 8, 9), \n",
      "k: 10, bic: -61.803, R^2: 0.645, subset: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), \n",
      "Best subset by exhaustive search: (0, 5, 7, 8, 9), bic: -72.366, R^2: 0.633\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def get_regression_results(predictor_subset, x, y):\n",
    "    # Use only a subset of predictors in the training data\n",
    "    x_subset = x[:, predictor_subset]\n",
    "\n",
    "    # Fit and evaluate R^2\n",
    "    model = sm.OLS(y, sm.add_constant(x_subset))\n",
    "\n",
    "    return model.fit()\n",
    "\n",
    "\n",
    "def get_best_k_subset(predictor_set, size_k, x, y):\n",
    "    # Create all possible subsets of size 'size',\n",
    "    # using the 'combination' function from the 'itertools' library\n",
    "    subsets_of_size_k = it.combinations(predictor_set, size_k)\n",
    "\n",
    "    max_r_squared = -float('inf')  # set some initial small value for max R^2 score\n",
    "    best_k_subset = []  # best subset of predictors of size k\n",
    "    best_results = None\n",
    "\n",
    "    # Iterate over all subsets of our predictor set\n",
    "    for predictor_subset in subsets_of_size_k:\n",
    "        results = get_regression_results(predictor_subset, x, y)\n",
    "        r_squared = results.rsquared\n",
    "\n",
    "        # Update max R^2 and best predictor subset of size k\n",
    "        # If current predictor subset has a higher R^2 score than that of the best subset\n",
    "        # we've found so far, remember the current predictor subset as the best!\n",
    "        if r_squared > max_r_squared:\n",
    "            max_r_squared = r_squared\n",
    "            best_k_subset = predictor_subset\n",
    "            best_results = results\n",
    "\n",
    "    return best_k_subset, best_results\n",
    "\n",
    "\n",
    "def get_results_stats(results):\n",
    "    return 'bic: %.3f, R^2: %.3f' % (results.bic, results.rsquared)\n",
    "\n",
    "\n",
    "def exhaustive_search_prob_1b(dataset_1_data):\n",
    "    x = dataset_1_data.x\n",
    "    y = dataset_1_data.y\n",
    "\n",
    "    # Best Subset Selection\n",
    "    min_bic = float('inf')  # set some initial large value for min BIC score\n",
    "    best_subset = []  # best subset of predictors\n",
    "    best_results = None\n",
    "\n",
    "    num_predictors = x.shape[1]\n",
    "\n",
    "    # Create all possible subsets of the set of <num_predictors> predictors\n",
    "    predictor_set = range(num_predictors)  # e.g. predictor set = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "    # Repeat for every possible size of subset\n",
    "    for size_k in range(1, num_predictors + 1):\n",
    "        k_subset, results = get_best_k_subset(predictor_set, size_k, x, y)\n",
    "\n",
    "        bic = results.bic\n",
    "\n",
    "        # Update minimum BIC and best predictor subset\n",
    "        # If current predictor has a lower BIC score than that of the best subset\n",
    "        # we've found so far, remember the current predictor as the best!\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            best_subset = k_subset\n",
    "            best_results = results\n",
    "\n",
    "        print 'k: %2d, %s, subset: %s, ' % (size_k, get_results_stats(results), str(k_subset))\n",
    "\n",
    "    print 'Best subset by exhaustive search: %s, %s' % (str(best_subset), get_results_stats(best_results))\n",
    "\n",
    "    \n",
    "exhaustive_search_prob_1b(dataset_1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 0, bic: -36.720, R^2: 0.369, subset: [8], \n",
      "size: 1, bic: -61.732, R^2: 0.531, subset: [8, 9], \n",
      "size: 2, bic: -65.729, R^2: 0.570, subset: [8, 9, 5], \n",
      "size: 3, bic: -71.008, R^2: 0.610, subset: [8, 9, 5, 7], \n",
      "size: 4, bic: -72.366, R^2: 0.633, subset: [8, 9, 5, 7, 0], \n",
      "size: 5, bic: -70.355, R^2: 0.642, subset: [8, 9, 5, 7, 0, 3], \n",
      "size: 6, bic: -66.159, R^2: 0.644, subset: [8, 9, 5, 7, 0, 3, 6], \n",
      "size: 7, bic: -61.803, R^2: 0.645, subset: [8, 9, 5, 7, 0, 3, 6, 4], \n",
      "size: 8, bic: -61.803, R^2: 0.645, subset: [8, 9, 5, 7, 0, 3, 6, 4, 1], \n",
      "size: 9, bic: -61.803, R^2: 0.645, subset: [8, 9, 5, 7, 0, 3, 6, 4, 1, 2], \n",
      "Best step-wise forward subset selection: [0, 5, 7, 8, 9], bic: -61.803, R^2: 0.645\n"
     ]
    }
   ],
   "source": [
    "def find_best_predictor_to_add(current_predictors, remaining_predictors, x, y):\n",
    "    max_r_squared = -float('inf')  # set some initial small value for max R^2\n",
    "    best_predictor = -1  # set some throwaway initial number for the best predictor to add\n",
    "    best_results = None\n",
    "\n",
    "    # Iterate over all remaining predictors to find best predictor to add\n",
    "    for i in remaining_predictors:\n",
    "        # Make copy of current set of predictors\n",
    "        predictor_subset = current_predictors[:]\n",
    "        # Add predictor 'i'\n",
    "        predictor_subset.append(i)\n",
    "\n",
    "        results = get_regression_results(predictor_subset, x, y)\n",
    "        r_squared = results.rsquared\n",
    "\n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if r_squared > max_r_squared:\n",
    "            max_r_squared = r_squared\n",
    "            best_predictor = i\n",
    "            best_results = results\n",
    "\n",
    "    return best_predictor, best_results\n",
    "\n",
    "\n",
    "def stepwise_forward_selection_prob_1b(dataset_1_data):\n",
    "    x = dataset_1_data.x\n",
    "    y = dataset_1_data.y\n",
    "\n",
    "    # Step-wise Forward Selection\n",
    "    d = x.shape[1]  # total no. of predictors\n",
    "\n",
    "    # Keep track of current set of chosen predictors, and the remaining set of predictors\n",
    "    current_predictors = []\n",
    "    remaining_predictors = range(d)\n",
    "\n",
    "    # Set some initial large value for min BIC score for all possible subsets\n",
    "    min_bic = float('inf')\n",
    "    results = None\n",
    "\n",
    "    # Keep track of the best subset of predictors\n",
    "    best_subset = []\n",
    "\n",
    "    # Iterate over all possible subset sizes, 0 predictors to d predictors\n",
    "    for size in range(d):\n",
    "        predictor, results = find_best_predictor_to_add(current_predictors, remaining_predictors, x, y)\n",
    "\n",
    "        # Remove best predictor from remaining list, and add best predictor to current list\n",
    "        remaining_predictors.remove(predictor)\n",
    "        current_predictors.append(predictor)\n",
    "\n",
    "        print 'size: %d, %s, subset: %s, ' % (size, get_results_stats(results), str(current_predictors))\n",
    "\n",
    "        # Check if BIC for with the predictor we just added is lower than\n",
    "        # the global minimum across all subset of predictors\n",
    "        if results.bic < min_bic:\n",
    "            best_subset = current_predictors[:]\n",
    "            min_bic = results.bic\n",
    "            results = results\n",
    "\n",
    "    print 'Best step-wise forward subset selection: %s, %s' % (str(sorted(best_subset)), get_results_stats(results))\n",
    "\n",
    "    \n",
    "stepwise_forward_selection_prob_1b(dataset_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?***\n",
    "\n",
    "Both methods returned the same set of predictors (x1, x6, x8, x9, x10) and they match the criteria set in part (a), that is x1 is one predictor from the first group of predictors correlated to each other (x1, x2, x3) and x6 is from the second group (x4, x5, x6, x7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Lasso and Ridge regression\n",
    "\n",
    "- Apply Lasso regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Apply Ridge regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Is there a difference between the model parameters you obtain different and those obtained from Lasso regression? If so, explain why.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to perform Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "Coefficients: [ 0.02717417  0.          0.         -0.         -0.02532806 -0.         -0.\n",
      "  0.04397321 -0.40612185 -0.22260474]\n",
      "Predictors with non-zero coefficients: [0, 4, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def lasso_regression_prob_1c(dataset_1_data):\n",
    "    x = dataset_1_data.x\n",
    "    y = dataset_1_data.y\n",
    "\n",
    "    # Lasso regression\n",
    "    reg = Lasso_Reg(alpha=0.01)\n",
    "    reg.fit(x, y)\n",
    "    coefficients = reg.coef_\n",
    "\n",
    "    print 'Lasso:'\n",
    "    print 'Coefficients:', coefficients\n",
    "    print 'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]\n",
    "\n",
    "\n",
    "lasso_regression_prob_1c(dataset_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset of predictors chosen by Lasso agrees with the correlation matrix we visualize in Part (a). That is, the best predictor subset contains five predictors, with **one predictor from each group of correlated predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge:\n",
      "Coefficients: [ 0.04353543  0.04353543  0.04353543  0.55217415 -0.19706852 -0.61421737\n",
      "  0.30484213  0.18742866 -0.50083242 -0.35908145]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression_prob_1c(dataset_1_data):\n",
    "    x = dataset_1_data.x\n",
    "    y = dataset_1_data.y\n",
    "\n",
    "    # Ridge regression: Fit and evaluate\n",
    "    reg = Ridge_Reg(alpha=0.01)\n",
    "    reg.fit(x, y)\n",
    "    coefficients = reg.coef_\n",
    "\n",
    "    print 'Ridge:'\n",
    "    print 'Coefficients:', coefficients\n",
    "    print 'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]\n",
    "\n",
    "\n",
    "ridge_regression_prob_1c(dataset_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is there a difference between the model parameters obtained from Ridge regression and those obtained from Lasso regression? If so, explain why.***\n",
    "\n",
    "The ridge regression model coefficients are all non-zero, whereas some of the coefficients obtained from the lasso regression equal zero. The ridge regression coefficients are getting closer to zero as the tuning parameter gets larger. In contrast lasso regression model coefficients can become zero as the tuning/regularization parameter gets larger.\n",
    "\n",
    "***Identify the predictors that are assigned non-zero coefficients. Do these correspond to the correlation matrix in Part (a)?***\n",
    "\n",
    "All predictors have non-zero coefficients. The ridge regression doesn't do variable selection.\n",
    "\n",
    "***Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation.***\n",
    "\n",
    "For ridge regression the first three predictors have exactly the same coefficients. For the lasso regression only of those coefficients are non-zero. This is because the lasso regression can do variable selection and ridge regression doesn't. It only makes sure that the coefficients are within the limits set by the tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Cross-validation and Bootstrapping\n",
    "In this problem, you will work with an expanded version of the automobile pricing data set you analyzed in Homework 3. The data set is contained ``dataset_2.txt``, with 26 attribues (i.e. predictors) for each automobile and corresponding prices. \n",
    "\n",
    "### Part(a): Encode categorical attributes and fill missing values\n",
    "Identify the categorical attributes in the data. Replace their values with the one-hot binary encoding. You may do this using the `get_dummies()` function in `pandas`. If you do this task correctly, you should get a total of 69 predictors after the encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 columns:\n",
      "Index([u'horsepower', u'highway-mpg', u'normalized-losses', u'wheel-base',\n",
      "       u'length', u'width', u'height', u'curb-weight', u'engine-size', u'bore',\n",
      "       u'stroke', u'compression-ratio', u'peak-rpm', u'city-mpg',\n",
      "       u'symboling_-2.0', u'symboling_0.0', u'symboling_1.0',\n",
      "       u'symboling_1.09604519774', u'symboling_2.0', u'symboling_3.0',\n",
      "       u'make_audi', u'make_bmw', u'make_chevrolet', u'make_dodge',\n",
      "       u'make_honda', u'make_mazda', u'make_mercedes-benz', u'make_mercury',\n",
      "       u'make_mitsubishi', u'make_nissan', u'make_peugot', u'make_plymouth',\n",
      "       u'make_porsche', u'make_renault', u'make_saab', u'make_subaru',\n",
      "       u'make_toyota', u'make_volkswagen', u'make_volvo', u'fuel-type_diesel',\n",
      "       u'fuel-type_gas', u'aspiration_std', u'aspiration_turbo',\n",
      "       u'num-of-doors_four', u'num-of-doors_two', u'body-style_convertible',\n",
      "       u'body-style_hardtop', u'body-style_hatchback', u'body-style_sedan',\n",
      "       u'body-style_wagon', u'drive-wheels_4wd', u'drive-wheels_fwd',\n",
      "       u'drive-wheels_rwd', u'engine-location_front', u'engine-location_rear',\n",
      "       u'engine-type_dohc', u'engine-type_l', u'engine-type_ohc',\n",
      "       u'engine-type_ohcf', u'engine-type_ohcv', u'num-of-cylinders_eight',\n",
      "       u'num-of-cylinders_five', u'num-of-cylinders_four',\n",
      "       u'num-of-cylinders_six', u'fuel-system_1bbl', u'fuel-system_2bbl',\n",
      "       u'fuel-system_idi', u'fuel-system_mpfi', u'fuel-system_spdi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Dataset_2_Data = namedtuple('Dataset_2_Data', ['train', 'test'])\n",
    "\n",
    "XY_Data = namedtuple('XY_Data', ['x', 'y'])\n",
    "\n",
    "\n",
    "def split(df, split_index):\n",
    "    train = df[:split_index]\n",
    "    test = df[split_index:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def is_categorical(column):\n",
    "    return column.dtype == object or len(column.unique()) < 8\n",
    "\n",
    "\n",
    "def encode_categorical_variables_prob_2a():\n",
    "    df = pd.read_csv('datasets/dataset_2.txt')\n",
    "\n",
    "    y = df['price']\n",
    "    x_df = df.drop('price', axis=1)\n",
    "\n",
    "    expanded_x_df = x_df.copy()\n",
    "    for column_name in df.columns:\n",
    "        column = df[column_name]\n",
    "        if is_categorical(column):\n",
    "            dummies_df = pd.get_dummies(column, prefix=column_name)\n",
    "\n",
    "            expanded_x_df = expanded_x_df.drop(column_name, axis=1)\n",
    "            expanded_x_df = pd.concat([expanded_x_df, dummies_df], axis=1)\n",
    "\n",
    "    split_index = len(expanded_x_df) // 4\n",
    "\n",
    "    x_train, x_test = split(expanded_x_df, split_index)\n",
    "    y_train, y_test = split(y, split_index)\n",
    "\n",
    "    train = XY_Data(x_train, y_train)\n",
    "    test = XY_Data(x_test, y_test)\n",
    "\n",
    "    return Dataset_2_Data(train, test)\n",
    "\n",
    "\n",
    "def print_expanded_df_prob_2a(dataset_2_data):\n",
    "    x = dataset_2_data.train.x\n",
    "\n",
    "    print '%d columns:' % len(x.columns)\n",
    "    print x.columns\n",
    "\n",
    "    \n",
    "dataset_2_data = encode_categorical_variables_prob_2a()\n",
    "print_expanded_df_prob_2a(dataset_2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Apply regular linear regression\n",
    "- Split the data set into train and test sets, with the first 25% of the data for training and the remaining for testing.  \n",
    "\n",
    "\n",
    "- Use regular linear regression to fit a model to the training set and evaluate the R^2 score of the fitted model on both the training and test sets. What do you observe about these values?\n",
    "\n",
    "\n",
    "- You had seen in class that the R^2 value of a least-squares fit to a data set would lie between 0 and 1. Is this true for the test R^2 values reported above? If not, give a reason for why this is the case.\n",
    "\n",
    "\n",
    "- Is there a need for regularization while fitting a linear model to this data set?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Ridge regression\n",
    "\n",
    "- Apply Ridge regression on the training set for different values of the regularization parameter $\\lambda$ in the range $\\{10^{-7}, 10^{-6}, \\ldots, 10^7\\}$. Evaluate the R^2 score for the models you obtain on both the train and test sets. Plot both values as a function of $\\lambda$. \n",
    "\n",
    "\n",
    "- Explain the relationship between the regularization parameter and the training and test R^2 scores.\n",
    "\n",
    "\n",
    "- How does the best test R^2 value obtained using Ridge regression compare with that of plain linear regression? Explain.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a ridge regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d): Tune regularization parameter using cross-validation and bootstrapping\n",
    "-  Evaluate the performance of the Ridge regression for different regularization parameters $\\lambda$ using 5-fold cross validation **or** bootstrapping on the training set. \n",
    "\n",
    "    - Plot the cross-validation (CV) or bootstrapping R^2 score as a function of $\\lambda$. \n",
    "    \n",
    "    - How closely does the CV score or bootstrapping score match the R^2 score on the test set? Does the model with lowest CV score or bootstrapping score correspond to the one with maximum R^2 on the test set?\n",
    "    \n",
    "    - Does the model chosen by CV or bootstrapping perform better than plain linear regression?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits. You may also use `kFold` from `sklearn.cross_validation`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Ridge regression *via* ordinary least-squares regression\n",
    "\n",
    "We present an approach to implement Ridge regression using oridinary least-squares regression. Given a matrix of responses $\\mathbf{X} \\in \\mathbb{R}^{n\\times p}$ and response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$, one can implement Ridge regression with regularization parameter $\\lambda$ as follows:\n",
    "\n",
    "- Augment the matrix of predictors $\\mathbf{X}$ with $p$ new rows containing the scaled identity matrix $\\sqrt{\\lambda}\\mathbf{I} \\in \\mathbb{R}^{p \\times p}$, i.e.\n",
    "$$\\overline{\\mathbf{X}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "X_{11} & \\ldots & X_{1p}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "X_{n1} & \\ldots & X_{np}\\\\\n",
    "\\sqrt{\\lambda} & \\ldots & 0\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "0 & \\ldots & \\sqrt{\\lambda}\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{(n+p)\\times p}\n",
    ".\n",
    "$$\n",
    "\n",
    "\n",
    "- Augment the response vector $\\mathbf{y}$ with a column of $p$ zeros, i.e.\n",
    "$$\n",
    "\\overline{\\mathbf{y}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{n+p}.\n",
    "$$\n",
    "\n",
    "\n",
    "- Apply ordinary least-squares regression on the augmented data set $(\\overline{\\mathbf{X}}, \\overline{\\mathbf{y}})$.\n",
    "\n",
    "### Part (a): Show the proposed approach implements Ridge regression\n",
    "Show that the approach proposed above implements Ridge regression with parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Debug our implementation of ridge regression\n",
    "You're a grader for CS109A, the following is an implemention of Ridge regression (via the above approach) submitted by a student. The dataset is ``dataset_3.txt``. The regression model is fitted to a training set, and the R^2 scores of the fitted model on the training and test sets are plotted as a function of the regularization parameter. Grade this solution according to the following rubric (each category is equally weighted): \n",
    "\n",
    "- correctness\n",
    "\n",
    "- interpretation (if applicable)\n",
    "\n",
    "- code/algorithm design\n",
    "\n",
    "- presentation\n",
    "\n",
    "In addition to providing an holistic grade (between 0 to 5), provide a corrected version of this code that is submission quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1149841d0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114906290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit\n",
    "def ridge(x_train, y_train, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_train=np.concatenate((x_train,reg_param*np.identity(n)),axis=1)\n",
    "    y_train_=np.zeros((n+np.shape(x_train)[1],1))\n",
    "    for c in range(n):\n",
    "        y_train_[c]= y_train[c]\n",
    "    import sklearn\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "    model.fit(x_train,y_train.reshape(-1,1))\n",
    "    return model\n",
    "\n",
    "# Score\n",
    "def score(m,x_test,y_test, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_test=np.concatenate((x_test,reg_param*np.identity(n)),axis=1)\n",
    "    y_test_=np.zeros((n+np.shape(x_test)[1],1))\n",
    "    for c in range(n):\n",
    "        y_test_[c]= y_test[c]\n",
    "    return m.score(x_test,y_test.reshape(-1,1))\n",
    "\n",
    "# Load\n",
    "data = np.loadtxt('datasets/dataset_3.txt', delimiter=',')\n",
    "n = data.shape[0]\n",
    "n = int(np.round(n*0.5))\n",
    "x_train = data[0:n,0:100]\n",
    "y_train = data[0:n,100]\n",
    "x_test = data[n:2*n,0:100]\n",
    "y_test = data[n:2*n,100]\n",
    "\n",
    "# Params\n",
    "a=np.zeros(5)\n",
    "for i in range(-2,2):\n",
    "    a[i+2]=10**i\n",
    "\n",
    "# Iterate\n",
    "rstr =np.zeros(5)\n",
    "rsts =np.zeros(5)\n",
    "for j in range(0,5):    \n",
    "    m =ridge(x_train,y_train,a[i])\n",
    "    rstr[j]=score(m,x_train,y_train,a[j])\n",
    "    rsts[i]=score(m,x_test,y_test,a[i])\n",
    "\n",
    "# Plot\n",
    "plt.plot(a,rstr)\n",
    "plt.plot(a,rsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Predicting Outcome of a Fund-raising Campaign\n",
    "You are provided a data set containing details of mail sent to 95,412 potential donors for a fund-raising campaign of a not-for-profit organization. This data set also contains the amount donated by each donor. The task is to build a model that can estimate the amount that a donor would donate using his/her attributes. The data is contained in the file `dataset_4.txt`. Each row contains 376 attributes for a donor, followed by the donation amount.\n",
    "\n",
    "**Note**: For additional information about the attributes used, please look up the file `dataset_4_description.txt`. This files also contains details of attributes that have been omitted from the data set.\n",
    "\n",
    "### Part (a): Fit regression model\n",
    "Build a suitable model to predict the donation amount. How good is your model? \n",
    "\n",
    "\n",
    "### Part (b): Evaluate the total profit of the fitted model\n",
    "Suppose you are told that the cost of mailing the donor is \\$7. Use your model to maximize profit. Implement, explain and rigorously justify your strategy. How does your strategry compare with blanket mailing everyone.\n",
    "\n",
    "### Part (c): Further Discussion\n",
    "In hindsight, thoroughly discuss the appropriatenes of using a regression model for this dataset (you must at least address the suitability with respect to profit maximization and model assumptions). Rigorously justify your reasoning. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
